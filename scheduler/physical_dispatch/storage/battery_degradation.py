# Computes the battery degradation following ESPA model formulations,# which is based on Rosewater, et al 2019import numpy as npimport osfrom gams import GamsWorkspaceimport pandas as pdimport datetimeclass Degradation:    def __init__(self, uid_st, uid_end, storage_df,                 run_dir='.'):        self.dispatch_dir = os.path.join(run_dir,'physical_dispatch/storage/results/')        self.uid_st = uid_st        self.uid_ed = uid_end        self.rids = storage_df.loc[:,'rid'].values        self.K_time = storage_df.loc[:,'deg_time'].values        self.K_therm = storage_df.loc[:,'deg_therm'].values        self.K_soc = storage_df.loc[:,'deg_soc'].values        self.C_EoL = storage_df.loc[:,'cost_EoL'].values        self.A0 = storage_df.loc[:,'deg_DoD0'].values        self.A1 = storage_df.loc[:,'deg_DoD1'].values        self.A2 = storage_df.loc[:,'deg_DoD2'].values        self.A3 = storage_df.loc[:,'deg_DoD3'].values        self.A4 = storage_df.loc[:,'deg_DoD4'].values        self.A_Tref = storage_df.loc[:,'temp_ref'].values        self.soc_capacity = np.array((storage_df.loc[:,'soc_capacity'].values))        self.A_socref = np.array((storage_df.loc[:,'socref'].values))/self.soc_capacity        self.num_stack = storage_df.loc[:,'cell_count'].values    def compute_degradation_cost(self):        '''        Looks for files between the start and end times,         then uses those and battery attributes to compute degradation cost        '''        # Select proper dispatch results files        dispatch_dir = self.dispatch_dir        ws = GamsWorkspace(working_directory=os.getcwd())        file_list = [f for f in os.listdir(dispatch_dir) if \                     (os.path.isfile(os.path.join(dispatch_dir,f)) and '.gdx' in f)]        file_list.sort() # Order by time        [idx_start, idx_end] = [i for i, filename in enumerate(file_list) \                                if (self.uid_st in filename) or (self.uid_ed in filename)]        files = file_list[idx_start:idx_end] # For now exclude end time so we don't double count        # Now fill time, soc, and temp arrays from input files        self._times = np.zeros(len(files), dtype=datetime.datetime)        self.socs = np.zeros((len(self.rids), len(files)))        self.temps = np.zeros((len(self.rids), len(files)))        for i, file in enumerate(files):            gams_db = ws.add_database_from_gdx(os.path.join(dispatch_dir,file))            soc_records = gams_db["soc"]            temp_records = gams_db["temp"]            for k, rid in enumerate(self.rids):                try:                    self.socs[k,i] = soc_records.find_record(rid).value/self.soc_capacity[k]                    self.temps[k,i] = temp_records.find_record(rid).value                except:                    self.socs[k,i] = 0                    self.temps[k,i] = 0            tidx = [j for j, char in enumerate(file.split('.')[0]) if char == '2'][0]            self._times[i] = datetime.datetime.strptime(file.split('.')[0][tidx:],'%Y%m%d%H%M')        # Make times an array of differences then convert to minutes        self._times -= self._times[0]        self.times = np.zeros(len(files))        for i in range(len(self.times)):            # Time in Rosewater paper is in units of hours            self.times[i] = self._times[i].total_seconds() / 3600        degradation_costs = {} # np.zeros(len(self.rids))              for k in range(len(self.rids)):            # Find the average soc and temperature            self.soc = np.sum(self.socs[k,:])/len(self.socs[k,:]) #/(self.times[-1]-self.times[0])            self.temp = np.sum(self.temps[k,:])/len(self.temps[k,:]) #/(self.times[-1]-self.times[0])            # Compute all of the parameters, then find the degradation cost            self.calculate_parameters(k)            # Eqn 120 (give or take) in model_formulations_v1-1            degradation_costs[self.rids[k]] = self.C_EoL[k] * self.dSoH_dt * (self.times[-1]-self.times[0])        return degradation_costs        def calculate_parameters(self, idx):        self.F_t = self.K_time[idx]*(self.times[-1]-self.times[0])        self.F_T = np.exp(self.K_therm[idx]*(self.temp-self.A_Tref[idx])*self.A_Tref[idx]/self.temp)        self.F_S = np.exp(self.K_soc[idx]*(self.soc-self.A_socref[idx]))        self.D_vec = self.rainflow(idx)        self.delta = 1/np.sum(self.socs[idx,:])*np.sum(self.D_vec)        # self.delta = np.sum(self.D_vec)/len(self.D_vec)        self.F_D = self.A0[idx] + self.A1[idx]*self.delta + self.A2[idx]*self.delta**2 + \            self.A3[idx]*self.delta**3 + self.A4[idx]*self.delta**4        self.f = self.F_t*self.F_S*self.F_T + np.sum(self.D_vec)*self.F_D*self.F_S*self.F_T        self.dSoH_dt = -self.K_time[idx]*self.F_S*self.F_T*np.exp(-self.f)            def rainflow(self, idx):        # 1: soc profile is self.socs        # 2: remove all non-peak, non-trough values (keeping 1st, last elements)        delete_inds = []        on_plateau = False        incoming_dlt = 0        tol = 0.0001        for i in range(len(self.socs[idx,:])):            if i == 0 or i == len(self.socs[idx,:])-1:                continue            dlt1 = (self.socs[idx,i] - self.socs[idx,i-1])            dlt2 = (self.socs[idx,i+1] - self.socs[idx,i])            # Make sure floating point errors don't count as sign changes            if abs(dlt1) < tol:                dlt1 = 0            if abs(dlt2) < tol:                dlt2 = 0            is_max_min = (dlt1*dlt2)<0            # Logic to handle soc plateau peaks/troughs            if on_plateau and (incoming_dlt*dlt2) < 0:                is_max_min = True            if dlt2 == 0:                if not on_plateau:                    incoming_dlt = dlt1                on_plateau = True            else:                on_plateau = False            if not is_max_min:                delete_inds += [i]        _socs = np.delete(self.socs[idx,:], delete_inds)        # Check that it didn't end on a plateau. If it did, delete the extra        if abs(_socs[-1] - _socs[-2]) < tol:            _socs = np.delete(_socs, -1)        # 3: Reorder so highest peak occurs first and append peak to be the last element        max_idx = np.argmax(_socs)        max_val = _socs[max_idx]        mask = np.arange(len(_socs),dtype=int)+max_idx        mask[mask>=len(mask)] -= len(mask)        _socs = _socs[mask]        _socs = np.append(_socs, max_val)        _socs = list(_socs)        # 4: Create empty vectors R and D (using lists, convert to array at end)        R, D = [], []        while len(_socs) > 0:            if len(_socs)+len(R) < 3:                break            # 5, 6: remove first element from s and insert it at the beginning of R until len(R)==3            while len(R) < 3:                R += [_socs[0]]                _socs.remove(_socs[0])            # 7: Set X and Y as the magnitude differences            X, Y = abs(R[0]-R[1]), abs(R[1]-R[2])            # 8: Check if X >= Y. If so, add Y to vector D and remove 2nd, 3rd elements from R            if X >= Y:                D += [Y]            R = [R[0]]            # 9: Repeat until done        if len(D) == 0:            D = 0        return D    # TODO: The below is just for testing - delete when doneif __name__ == '__main__':    espa_dir = "/Users/corn677/Projects/WEASLE/code/espa-comp"    resource_file = os.path.join(espa_dir,'market_clearing/offer_data','resource_info.xlsx')    resource_str = pd.read_excel(resource_file, sheet_name='Storage')    # attributes_dict = {}    # for col in resource_str.columns.values:    #     attributes_dict[col] = resource_str.at[0,col]    uid_start = 'TSRTM201801280000'    uid_end = 'TSRTM201801290000'    battery_deg = Degradation(uid_start, uid_end, resource_str)    deg_costs = battery_deg.compute_degradation_cost()    print("Degradation Costs:", deg_costs)